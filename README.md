# Docker compose generator for Spark and Cassandra

## Quick-start guide

1. Install Python3, you can use [pyenv](https://github.com/pyenv/pyenv) for this
2. Install requrements: `python3 -m pip install -r requirements.txt`
3. Generate an docker-compose.yml: `./compose-generator.py --replicas=2 --cassandra-cpu=0.5 --cassandra-mem=1024M --spark-cpu=2 --spark-mem=2048M`
4. Run docker-compose: `docker-compose up`

## Work with Spark

Spark web UI is accesible on the localhost: http://127.0.0.1:8080/
This setup also uses [spark-cassandra-connector](https://github.com/datastax/spark-cassandra-connector) for accessing Cassandra from Spark. You can use the Spark Cassandra Connector with the Spark Shell:
1. Exec to the docker container with Spark Master: `docker exec -it compose_spark_1 /bin/bash`
2. Run the Spark Shell with the Spark Cassandra Connector: `./bin/spark-shell --master spark://spark:7077 --jars jars/spark-cassandra-connector-assembly_2.12-3.1.0.jar --packages com.datastax.spark:spark-cassandra-connector_2.12:3.1.0 --conf spark.cassandra.connection.host=cassandra-1,cassandra-2 --conf spark.cassandra.auth.username=cassandra --conf spark.cassandra.auth.password=cassandra`
More info could be found [here](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/13_spark_shell.md)

## Script parameters
```
  -h, --help            show help message and exit
  --replicas REPLICAS
                        Cassandra and Spark workers number
  --cassandra-cpu CASSANDRA_CPU
                        CPU limit for Cassandra nodes
  --cassandra-mem CASSANDRA_MEM
                        Memory limit for Cassandra nodes
  --spark-cpu SPARK_CPU
                        CPU limit for Spark nodes (only an integer number is required)
  --spark-mem SPARK_MEM
                        Memory limit for Spark nodes
```

## Security concerns

You should use a docker-compose.yml generated by this script only for testing purposes. It doesn't setup authentification for the Spark cluster and uses a week password for the Cassandra Cluster